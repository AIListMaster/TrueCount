{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:46:32.590761833Z",
     "start_time": "2024-02-27T12:46:29.580433017Z"
    }
   },
   "id": "b39308e0b1daad7e",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "                                                text  rating\n0  Visited the place with family friends to savou...       2\n1  We went there for a friend's bday. The food wa...       2\n2  Sindhi Sweets in sector 11 in Panchkula is ver...       2\n3  Me and my sister visited there for some snacki...       2\n4  Conveniently located in the sector 11 market, ...       2",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Visited the place with family friends to savou...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>We went there for a friend's bday. The food wa...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Sindhi Sweets in sector 11 in Panchkula is ver...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Me and my sister visited there for some snacki...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Conveniently located in the sector 11 market, ...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "# Assuming your dataset is in a CSV file\n",
    "df = pd.read_csv('reviews.csv')\n",
    "df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:54:50.057405247Z",
     "start_time": "2024-02-27T12:54:50.039757222Z"
    }
   },
   "id": "fb9c36673960baf9",
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "1950    2\n602     1\n986     2\n596     2\n693     2\n       ..\n1638    0\n1095    2\n1130    2\n1294    2\n860     2\nName: rating, Length: 1928, dtype: int64"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into training and testing sets\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df['rating']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:54:53.253237344Z",
     "start_time": "2024-02-27T12:54:53.247331367Z"
    }
   },
   "id": "e27a5d482c305904",
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "\n",
    "# Define a custom dataset class for loading data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',\n",
    "            truncation=True\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:54:55.942277548Z",
     "start_time": "2024-02-27T12:54:55.934452908Z"
    }
   },
   "id": "97d0e8760cf51686",
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": "BertForSequenceClassification(\n  (bert): BertModel(\n    (embeddings): BertEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(512, 768)\n      (token_type_embeddings): Embedding(2, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): BertEncoder(\n      (layer): ModuleList(\n        (0-11): 12 x BertLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): BertPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (classifier): Linear(in_features=768, out_features=3, bias=True)\n)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up BERT model and tokenizer\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:54:58.218519704Z",
     "start_time": "2024-02-27T12:54:56.983669514Z"
    }
   },
   "id": "e47c2a06489d450d",
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create DataLoader for training and testing sets\n",
    "max_len = 128\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = CustomDataset(train_df['text'].values, train_df['rating'].values, tokenizer, max_len)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = CustomDataset(test_df['text'].values, test_df['rating'].values, tokenizer, max_len)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T12:55:13.639390245Z",
     "start_time": "2024-02-27T12:55:13.595822345Z"
    }
   },
   "id": "a99bb700168f4f3b",
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/uditrawat/Projects/Projects-Udit/AI-Projects/P-TrueCount/TrueCount/.venv/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Set up optimizer and loss function\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:15:21.555436553Z",
     "start_time": "2024-02-27T12:55:19.202454174Z"
    }
   },
   "id": "c3dd7af51818618d",
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "model.eval()\n",
    "predictions = []\n",
    "true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        predictions.extend(torch.argmax(logits, dim=1).cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:28:53.525778832Z",
     "start_time": "2024-02-27T13:28:22.060097939Z"
    }
   },
   "id": "79cee5662be66c21",
   "execution_count": 25
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8921161825726142\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "print(f'Accuracy: {accuracy}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:29:00.675868259Z",
     "start_time": "2024-02-27T13:29:00.627900539Z"
    }
   },
   "id": "e9547b029030f9e3",
   "execution_count": 26
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "('/home/uditrawat/Projects/Projects-Udit/AI-Projects/P-TrueCount/TrueCount/apps/save/tokenizer/tokenizer_config.json',\n '/home/uditrawat/Projects/Projects-Udit/AI-Projects/P-TrueCount/TrueCount/apps/save/tokenizer/special_tokens_map.json',\n '/home/uditrawat/Projects/Projects-Udit/AI-Projects/P-TrueCount/TrueCount/apps/save/tokenizer/vocab.txt',\n '/home/uditrawat/Projects/Projects-Udit/AI-Projects/P-TrueCount/TrueCount/apps/save/tokenizer/added_tokens.json')"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "#model.save_pretrained('/home/uditrawat/Projects/Projects-Udit/AI-Projects/P-TrueCount/TrueCount/apps/save/model')\n",
    "#tokenizer.save_pretrained('/home/uditrawat/Projects/Projects-Udit/AI-Projects/P-TrueCount/TrueCount/apps/save/tokenizer')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-27T13:59:09.741137980Z",
     "start_time": "2024-02-27T13:59:09.062286635Z"
    }
   },
   "id": "a9542e8acbada470",
   "execution_count": 27
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "9107cfe049aa3ecd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
